<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>degann.networks.layers.tf_dense API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>degann.networks.layers.tf_dense</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">from typing import Optional, List, Tuple, Dict

import numpy as np
import tensorflow as tf
from tensorflow import keras

from degann.networks import activations
from degann.networks.config_format import LAYER_DICT_NAMES


def _dec_params_to_list(
    params: Optional[Dict[str, float]]
) -&gt; Optional[List[Tuple[str, float]]]:
    if params is None:
        return None
    res = []
    for key in params:
        res.append((key, params[key]))
    return res


def _dec_params_from_list(
    params: Optional[List[Tuple[str, float]]]
) -&gt; Optional[Dict[str, float]]:
    if params is None:
        return None
    res = {}
    for pair in params:
        res[pair[0]] = pair[1]
    return res


class TensorflowDense(keras.layers.Layer):
    def __init__(
        self,
        input_dim=32,
        units=32,
        activation_func: str = &#34;linear&#34;,
        weight_initializer=tf.random_normal_initializer(),
        bias_initializer=tf.random_normal_initializer(),
        is_debug=False,
        **kwargs,
    ):
        decorator_params = None

        if &#34;decorator_params&#34; in kwargs.keys():
            decorator_params = kwargs.get(&#34;decorator_params&#34;)
            kwargs.pop(&#34;decorator_params&#34;)

        if not isinstance(decorator_params, dict) and decorator_params is not None:
            raise &#34;Additional parameters for activation function must be dictionary&#34;

        if input_dim == 0 or units == 0:
            raise &#34;Layer cannot have zero inputs or zero size&#34;

        super(TensorflowDense, self).__init__(**kwargs)
        w_init = weight_initializer
        self.w = self.add_weight(
            shape=(input_dim, units),
            initializer=w_init,
            dtype=&#34;float32&#34;,
            # initial_value=w_init(shape=(input_dim, units), dtype=&#34;float32&#34;),
            name=f&#34;Var_w_{self.name}&#34;,
            trainable=True,
        )
        b_init = bias_initializer
        self.b = self.add_weight(
            shape=(units,),
            initializer=b_init,
            dtype=&#34;float32&#34;,
            # initial_value=b_init(shape=(units,), dtype=&#34;float32&#34;),
            name=f&#34;Var_b_{self.name}&#34;,
            trainable=True,
        )

        self.units = units
        self.input_dim = input_dim
        self._is_debug = is_debug
        self.activation_func = activations.get(activation_func)
        self.activation_name = activation_func
        self.weight_initializer = weight_initializer
        self.bias_initializer = bias_initializer
        self.decorator_params: Optional[dict] = decorator_params

    def call(self, inputs, **kwargs):
        &#34;&#34;&#34;
        Obtaining a layer response on the input data vector
        Parameters
        ----------
        inputs
        kwargs

        Returns
        -------

        &#34;&#34;&#34;
        if self.decorator_params is None:
            return self.activation_func(tf.matmul(inputs, self.w) + self.b)
        else:
            return self.activation_func(
                tf.matmul(inputs, self.w) + self.b, **self.decorator_params
            )

    def __str__(self):
        res = f&#34;Layer {self.name}\n&#34;
        res += f&#34;weights shape = {self.w.shape}\n&#34;
        if self._is_debug:
            # res += f&#34;weights = {self.w.numpy()}\n&#34;
            # res += f&#34;biases = {self.b.numpy()}\n&#34;
            res += f&#34;activation = {self.activation_name}\n&#34;
        return res

    def to_dict(self) -&gt; dict:
        &#34;&#34;&#34;
        Export layer to dictionary of parameters
        Returns
        -------
        config: dict
            dictionary of parameters
        &#34;&#34;&#34;
        w = self.w.value.numpy()
        b = self.b.value.numpy()
        res = {
            LAYER_DICT_NAMES[&#34;shape&#34;]: self.units,
            LAYER_DICT_NAMES[&#34;inp_size&#34;]: self.input_dim,
            LAYER_DICT_NAMES[&#34;weights&#34;]: w.tolist(),
            LAYER_DICT_NAMES[&#34;biases&#34;]: b.tolist(),
            LAYER_DICT_NAMES[&#34;layer_type&#34;]: type(self).__name__,
            LAYER_DICT_NAMES[&#34;dtype&#34;]: w.dtype.name,
            LAYER_DICT_NAMES[&#34;activation&#34;]: self.activation_name
            if self.activation_name is None
            else self.activation_name,
            LAYER_DICT_NAMES[&#34;decorator_params&#34;]: _dec_params_to_list(
                self.decorator_params
            ),
        }

        return res

    def from_dict(self, config):
        &#34;&#34;&#34;
        Restore layer from dictionary of parameters

        Parameters
        ----------
        config

        Returns
        -------

        &#34;&#34;&#34;
        w = np.array(config[LAYER_DICT_NAMES[&#34;weights&#34;]])
        b = np.array(config[LAYER_DICT_NAMES[&#34;biases&#34;]])
        act = config[LAYER_DICT_NAMES[&#34;activation&#34;]]
        dec_params = _dec_params_from_list(config[LAYER_DICT_NAMES[&#34;decorator_params&#34;]])
        self.set_weights([w, b])
        # self.b = tf.Variable(
        #     initial_value=b, dtype=config[LAYER_DICT_NAMES[&#34;dtype&#34;]], trainable=True
        # )
        self.activation_func = activations.get(act)
        self.activation_name = act
        self.decorator_params = dec_params

    @classmethod
    def from_config(cls, config):
        return cls(**config)

    @property
    def get_activation(self) -&gt; str:
        return self.activation_name</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="degann.networks.layers.tf_dense.TensorflowDense"><code class="flex name class">
<span>class <span class="ident">TensorflowDense</span></span>
<span>(</span><span>input_dim=32, units=32, activation_func: str = 'linear', weight_initializer=&lt;tensorflow.python.ops.init_ops_v2.RandomNormal object&gt;, bias_initializer=&lt;tensorflow.python.ops.init_ops_v2.RandomNormal object&gt;, is_debug=False, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>This is the class from which all layers inherit.</p>
<p>A layer is a callable object that takes as input one or more tensors and
that outputs one or more tensors. It involves <em>computation</em>, defined
in the <code>call()</code> method, and a <em>state</em> (weight variables). State can be
created:</p>
<ul>
<li>in <code>__init__()</code>, for instance via <code>self.add_weight()</code>;</li>
<li>in the optional <code>build()</code> method, which is invoked by the first
<code>__call__()</code> to the layer, and supplies the shape(s) of the input(s),
which may not have been known at initialization time.</li>
</ul>
<p>Layers are recursively composable: If you assign a Layer instance as an
attribute of another Layer, the outer layer will start tracking the weights
created by the inner layer. Nested layers should be instantiated in the
<code>__init__()</code> method or <code>build()</code> method.</p>
<p>Users will just instantiate a layer and then treat it as a callable.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>trainable</code></strong></dt>
<dd>Boolean, whether the layer's variables should be trainable.</dd>
<dt><strong><code>name</code></strong></dt>
<dd>String name of the layer.</dd>
<dt><strong><code>dtype</code></strong></dt>
<dd>The dtype of the layer's computations and weights. Can also be a
<code>keras.DTypePolicy</code>,
which allows the computation and
weight dtype to differ. Defaults to <code>None</code>. <code>None</code> means to use
<code>keras.config.dtype_policy()</code>,
which is a <code>float32</code> policy unless set to different value
(via <code>keras.config.set_dtype_policy()</code>).</dd>
</dl>
<h2 id="attributes">Attributes</h2>
<dl>
<dt><strong><code>name</code></strong></dt>
<dd>The name of the layer (string).</dd>
<dt><strong><code>dtype</code></strong></dt>
<dd>Dtype of the layer's weights. Alias of <code>layer.variable_dtype</code>.</dd>
<dt><strong><code>variable_dtype</code></strong></dt>
<dd>Dtype of the layer's weights.</dd>
<dt><strong><code>compute_dtype</code></strong></dt>
<dd>The dtype of the layer's computations.
Layers automatically cast inputs to this dtype, which causes
the computations and output to also be in this dtype.
When mixed precision is used with a
<code>keras.DTypePolicy</code>, this will be different
than <code>variable_dtype</code>.</dd>
<dt><strong><code>trainable_weights</code></strong></dt>
<dd>List of variables to be included in backprop.</dd>
<dt><strong><code>non_trainable_weights</code></strong></dt>
<dd>List of variables that should not be
included in backprop.</dd>
<dt><strong><code>weights</code></strong></dt>
<dd>The concatenation of the lists trainable_weights and
non_trainable_weights (in this order).</dd>
<dt><strong><code>trainable</code></strong></dt>
<dd>Whether the layer should be trained (boolean), i.e.
whether its potentially-trainable weights should be returned
as part of <code>layer.trainable_weights</code>.</dd>
<dt><strong><code>input_spec</code></strong></dt>
<dd>Optional (list of) <code>InputSpec</code> object(s) specifying the
constraints on inputs that can be accepted by the layer.</dd>
</dl>
<p>We recommend that descendants of <code>Layer</code> implement the following methods:</p>
<ul>
<li><code>__init__()</code>: Defines custom layer attributes, and creates layer weights
that do not depend on input shapes, using <code>add_weight()</code>,
or other state.</li>
<li><code>build(self, input_shape)</code>: This method can be used to create weights that
depend on the shape(s) of the input(s), using <code>add_weight()</code>, or other
state. <code>__call__()</code> will automatically build the layer
(if it has not been built yet) by calling <code>build()</code>.</li>
<li><code>call(self, *args, **kwargs)</code>: Called in <code>__call__</code> after making
sure <code>build()</code> has been called. <code>call()</code> performs the logic of applying
the layer to the input arguments.
Two reserved keyword arguments you can optionally use in <code>call()</code> are:
1. <code>training</code> (boolean, whether the call is in inference mode or
training mode).
2. <code>mask</code> (boolean tensor encoding masked timesteps in the input,
used e.g. in RNN layers).
A typical signature for this method is <code>call(self, inputs)</code>, and user
could optionally add <code>training</code> and <code>mask</code> if the layer need them.</li>
<li><code>get_config(self)</code>: Returns a dictionary containing the configuration
used to initialize this layer. If the keys differ from the arguments
in <code>__init__()</code>, then override <code>from_config(self)</code> as well.
This method is used when saving
the layer or a model that contains this layer.</li>
</ul>
<p>Examples:</p>
<p>Here's a basic example: a layer with two variables, <code>w</code> and <code>b</code>,
that returns <code>y = w . x + b</code>.
It shows how to implement <code>build()</code> and <code>call()</code>.
Variables set as attributes of a layer are tracked as weights
of the layers (in <code>layer.weights</code>).</p>
<pre><code class="language-python">class SimpleDense(Layer):
    def __init__(self, units=32):
        super().__init__()
        self.units = units

    # Create the state of the layer (weights)
    def build(self, input_shape):
        self.kernel = self.add_weight(
            shape=(input_shape[-1], self.units),
            initializer=&quot;glorot_uniform&quot;,
            trainable=True,
            name=&quot;kernel&quot;,
        )
        self.bias = self.add_weight(
            shape=(self.units,),
            initializer=&quot;zeros&quot;,
            trainable=True,
            name=&quot;bias&quot;,
        )

    # Defines the computation
    def call(self, inputs):
        return ops.matmul(inputs, self.kernel) + self.bias

# Instantiates the layer.
linear_layer = SimpleDense(4)

# This will also call `build(input_shape)` and create the weights.
y = linear_layer(ops.ones((2, 2)))
assert len(linear_layer.weights) == 2

# These weights are trainable, so they're listed in `trainable_weights`:
assert len(linear_layer.trainable_weights) == 2
</code></pre>
<p>Besides trainable weights, updated via backpropagation during training,
layers can also have non-trainable weights. These weights are meant to
be updated manually during <code>call()</code>. Here's a example layer that computes
the running sum of its inputs:</p>
<pre><code class="language-python">class ComputeSum(Layer):

  def __init__(self, input_dim):
      super(ComputeSum, self).__init__()
      # Create a non-trainable weight.
      self.total = self.add_weight(
        shape=(),
        initializer=&quot;zeros&quot;,
        trainable=False,
        name=&quot;total&quot;,
      )

  def call(self, inputs):
      self.total.assign(self.total + ops.sum(inputs))
      return self.total

my_sum = ComputeSum(2)
x = ops.ones((2, 2))
y = my_sum(x)

assert my_sum.weights == [my_sum.total]
assert my_sum.non_trainable_weights == [my_sum.total]
assert my_sum.trainable_weights == []
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class TensorflowDense(keras.layers.Layer):
    def __init__(
        self,
        input_dim=32,
        units=32,
        activation_func: str = &#34;linear&#34;,
        weight_initializer=tf.random_normal_initializer(),
        bias_initializer=tf.random_normal_initializer(),
        is_debug=False,
        **kwargs,
    ):
        decorator_params = None

        if &#34;decorator_params&#34; in kwargs.keys():
            decorator_params = kwargs.get(&#34;decorator_params&#34;)
            kwargs.pop(&#34;decorator_params&#34;)

        if not isinstance(decorator_params, dict) and decorator_params is not None:
            raise &#34;Additional parameters for activation function must be dictionary&#34;

        if input_dim == 0 or units == 0:
            raise &#34;Layer cannot have zero inputs or zero size&#34;

        super(TensorflowDense, self).__init__(**kwargs)
        w_init = weight_initializer
        self.w = self.add_weight(
            shape=(input_dim, units),
            initializer=w_init,
            dtype=&#34;float32&#34;,
            # initial_value=w_init(shape=(input_dim, units), dtype=&#34;float32&#34;),
            name=f&#34;Var_w_{self.name}&#34;,
            trainable=True,
        )
        b_init = bias_initializer
        self.b = self.add_weight(
            shape=(units,),
            initializer=b_init,
            dtype=&#34;float32&#34;,
            # initial_value=b_init(shape=(units,), dtype=&#34;float32&#34;),
            name=f&#34;Var_b_{self.name}&#34;,
            trainable=True,
        )

        self.units = units
        self.input_dim = input_dim
        self._is_debug = is_debug
        self.activation_func = activations.get(activation_func)
        self.activation_name = activation_func
        self.weight_initializer = weight_initializer
        self.bias_initializer = bias_initializer
        self.decorator_params: Optional[dict] = decorator_params

    def call(self, inputs, **kwargs):
        &#34;&#34;&#34;
        Obtaining a layer response on the input data vector
        Parameters
        ----------
        inputs
        kwargs

        Returns
        -------

        &#34;&#34;&#34;
        if self.decorator_params is None:
            return self.activation_func(tf.matmul(inputs, self.w) + self.b)
        else:
            return self.activation_func(
                tf.matmul(inputs, self.w) + self.b, **self.decorator_params
            )

    def __str__(self):
        res = f&#34;Layer {self.name}\n&#34;
        res += f&#34;weights shape = {self.w.shape}\n&#34;
        if self._is_debug:
            # res += f&#34;weights = {self.w.numpy()}\n&#34;
            # res += f&#34;biases = {self.b.numpy()}\n&#34;
            res += f&#34;activation = {self.activation_name}\n&#34;
        return res

    def to_dict(self) -&gt; dict:
        &#34;&#34;&#34;
        Export layer to dictionary of parameters
        Returns
        -------
        config: dict
            dictionary of parameters
        &#34;&#34;&#34;
        w = self.w.value.numpy()
        b = self.b.value.numpy()
        res = {
            LAYER_DICT_NAMES[&#34;shape&#34;]: self.units,
            LAYER_DICT_NAMES[&#34;inp_size&#34;]: self.input_dim,
            LAYER_DICT_NAMES[&#34;weights&#34;]: w.tolist(),
            LAYER_DICT_NAMES[&#34;biases&#34;]: b.tolist(),
            LAYER_DICT_NAMES[&#34;layer_type&#34;]: type(self).__name__,
            LAYER_DICT_NAMES[&#34;dtype&#34;]: w.dtype.name,
            LAYER_DICT_NAMES[&#34;activation&#34;]: self.activation_name
            if self.activation_name is None
            else self.activation_name,
            LAYER_DICT_NAMES[&#34;decorator_params&#34;]: _dec_params_to_list(
                self.decorator_params
            ),
        }

        return res

    def from_dict(self, config):
        &#34;&#34;&#34;
        Restore layer from dictionary of parameters

        Parameters
        ----------
        config

        Returns
        -------

        &#34;&#34;&#34;
        w = np.array(config[LAYER_DICT_NAMES[&#34;weights&#34;]])
        b = np.array(config[LAYER_DICT_NAMES[&#34;biases&#34;]])
        act = config[LAYER_DICT_NAMES[&#34;activation&#34;]]
        dec_params = _dec_params_from_list(config[LAYER_DICT_NAMES[&#34;decorator_params&#34;]])
        self.set_weights([w, b])
        # self.b = tf.Variable(
        #     initial_value=b, dtype=config[LAYER_DICT_NAMES[&#34;dtype&#34;]], trainable=True
        # )
        self.activation_func = activations.get(act)
        self.activation_name = act
        self.decorator_params = dec_params

    @classmethod
    def from_config(cls, config):
        return cls(**config)

    @property
    def get_activation(self) -&gt; str:
        return self.activation_name</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>keras.src.layers.layer.Layer</li>
<li>keras.src.backend.tensorflow.layer.TFLayer</li>
<li>keras.src.backend.tensorflow.trackable.KerasAutoTrackable</li>
<li>tensorflow.python.trackable.autotrackable.AutoTrackable</li>
<li>tensorflow.python.trackable.base.Trackable</li>
<li>keras.src.ops.operation.Operation</li>
<li>keras.src.saving.keras_saveable.KerasSaveable</li>
</ul>
<h3>Static methods</h3>
<dl>
<dt id="degann.networks.layers.tf_dense.TensorflowDense.from_config"><code class="name flex">
<span>def <span class="ident">from_config</span></span>(<span>config)</span>
</code></dt>
<dd>
<div class="desc"><p>Creates a layer from its config.</p>
<p>This method is the reverse of <code>get_config</code>,
capable of instantiating the same layer from the config
dictionary. It does not handle layer connectivity
(handled by Network), nor weights (handled by <code>set_weights</code>).</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>config</code></strong></dt>
<dd>A Python dictionary, typically the
output of get_config.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>A layer instance.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@classmethod
def from_config(cls, config):
    return cls(**config)</code></pre>
</details>
</dd>
</dl>
<h3>Instance variables</h3>
<dl>
<dt id="degann.networks.layers.tf_dense.TensorflowDense.get_activation"><code class="name">var <span class="ident">get_activation</span> : str</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def get_activation(self) -&gt; str:
    return self.activation_name</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="degann.networks.layers.tf_dense.TensorflowDense.call"><code class="name flex">
<span>def <span class="ident">call</span></span>(<span>self, inputs, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Obtaining a layer response on the input data vector
Parameters</p>
<hr>
<dl>
<dt><strong><code>inputs</code></strong></dt>
<dd>&nbsp;</dd>
<dt><strong><code>kwargs</code></strong></dt>
<dd>&nbsp;</dd>
</dl>
<h2 id="returns">Returns</h2></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def call(self, inputs, **kwargs):
    &#34;&#34;&#34;
    Obtaining a layer response on the input data vector
    Parameters
    ----------
    inputs
    kwargs

    Returns
    -------

    &#34;&#34;&#34;
    if self.decorator_params is None:
        return self.activation_func(tf.matmul(inputs, self.w) + self.b)
    else:
        return self.activation_func(
            tf.matmul(inputs, self.w) + self.b, **self.decorator_params
        )</code></pre>
</details>
</dd>
<dt id="degann.networks.layers.tf_dense.TensorflowDense.from_dict"><code class="name flex">
<span>def <span class="ident">from_dict</span></span>(<span>self, config)</span>
</code></dt>
<dd>
<div class="desc"><p>Restore layer from dictionary of parameters</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>config</code></strong></dt>
<dd>&nbsp;</dd>
</dl>
<h2 id="returns">Returns</h2></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def from_dict(self, config):
    &#34;&#34;&#34;
    Restore layer from dictionary of parameters

    Parameters
    ----------
    config

    Returns
    -------

    &#34;&#34;&#34;
    w = np.array(config[LAYER_DICT_NAMES[&#34;weights&#34;]])
    b = np.array(config[LAYER_DICT_NAMES[&#34;biases&#34;]])
    act = config[LAYER_DICT_NAMES[&#34;activation&#34;]]
    dec_params = _dec_params_from_list(config[LAYER_DICT_NAMES[&#34;decorator_params&#34;]])
    self.set_weights([w, b])
    # self.b = tf.Variable(
    #     initial_value=b, dtype=config[LAYER_DICT_NAMES[&#34;dtype&#34;]], trainable=True
    # )
    self.activation_func = activations.get(act)
    self.activation_name = act
    self.decorator_params = dec_params</code></pre>
</details>
</dd>
<dt id="degann.networks.layers.tf_dense.TensorflowDense.to_dict"><code class="name flex">
<span>def <span class="ident">to_dict</span></span>(<span>self) ‑> dict</span>
</code></dt>
<dd>
<div class="desc"><p>Export layer to dictionary of parameters
Returns</p>
<hr>
<dl>
<dt><strong><code>config</code></strong> :&ensp;<code>dict</code></dt>
<dd>dictionary of parameters</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def to_dict(self) -&gt; dict:
    &#34;&#34;&#34;
    Export layer to dictionary of parameters
    Returns
    -------
    config: dict
        dictionary of parameters
    &#34;&#34;&#34;
    w = self.w.value.numpy()
    b = self.b.value.numpy()
    res = {
        LAYER_DICT_NAMES[&#34;shape&#34;]: self.units,
        LAYER_DICT_NAMES[&#34;inp_size&#34;]: self.input_dim,
        LAYER_DICT_NAMES[&#34;weights&#34;]: w.tolist(),
        LAYER_DICT_NAMES[&#34;biases&#34;]: b.tolist(),
        LAYER_DICT_NAMES[&#34;layer_type&#34;]: type(self).__name__,
        LAYER_DICT_NAMES[&#34;dtype&#34;]: w.dtype.name,
        LAYER_DICT_NAMES[&#34;activation&#34;]: self.activation_name
        if self.activation_name is None
        else self.activation_name,
        LAYER_DICT_NAMES[&#34;decorator_params&#34;]: _dec_params_to_list(
            self.decorator_params
        ),
    }

    return res</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="degann.networks.layers" href="index.html">degann.networks.layers</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="degann.networks.layers.tf_dense.TensorflowDense" href="#degann.networks.layers.tf_dense.TensorflowDense">TensorflowDense</a></code></h4>
<ul class="">
<li><code><a title="degann.networks.layers.tf_dense.TensorflowDense.call" href="#degann.networks.layers.tf_dense.TensorflowDense.call">call</a></code></li>
<li><code><a title="degann.networks.layers.tf_dense.TensorflowDense.from_config" href="#degann.networks.layers.tf_dense.TensorflowDense.from_config">from_config</a></code></li>
<li><code><a title="degann.networks.layers.tf_dense.TensorflowDense.from_dict" href="#degann.networks.layers.tf_dense.TensorflowDense.from_dict">from_dict</a></code></li>
<li><code><a title="degann.networks.layers.tf_dense.TensorflowDense.get_activation" href="#degann.networks.layers.tf_dense.TensorflowDense.get_activation">get_activation</a></code></li>
<li><code><a title="degann.networks.layers.tf_dense.TensorflowDense.to_dict" href="#degann.networks.layers.tf_dense.TensorflowDense.to_dict">to_dict</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>